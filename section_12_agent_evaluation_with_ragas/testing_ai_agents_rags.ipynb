{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ba5e14",
   "metadata": {},
   "source": [
    "## Testing AI Agents & RAGs with RAGAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d94c07f",
   "metadata": {},
   "source": [
    "### Setup dotEnv and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59ee691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da79b99",
   "metadata": {},
   "source": [
    "### Setup the Embeddings & Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b596aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2:latest\", temperature=0.5)\n",
    "\n",
    "vector_store = Chroma(\n",
    "    persist_directory='./../Section12_AIAgentEvaluationWithRAGAs/chroma_langchain_db',\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cdc8c9",
   "metadata": {},
   "source": [
    "### Import CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7e0a4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "data = read_csv('./dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8eee4",
   "metadata": {},
   "source": [
    "### Tool creation for Bias Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8b380a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def bias_detection(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect bias in the given article and summarize findings in 5 bullet points.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query related to bias in LLM.\n",
    "    \n",
    "    Returns:\n",
    "        A string containing 5 bullet points summarizing the bias-related findings.\n",
    "    \"\"\"\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    \n",
    "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    The following text discusses potential biases in LLMs:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Please extract and summarize the bias-related points in exactly **five bullet points**.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a summary using LLM\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return response.content "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9724671b",
   "metadata": {},
   "source": [
    "### Setting-up the AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "371fa464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "list_of_tools = [bias_detection]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=list_of_tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5250a61",
   "metadata": {},
   "source": [
    "### Preparing the Dataset for RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1a9df804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": {\n",
      "    \"query\": {\n",
      "      \"title\": \"Bias in description of Asian man\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  }\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": {\"title\": \"Men being strong\", \"type\": \"sentence\"}\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": {\n",
      "    \"query\": {\n",
      "      \"title\": \"Hamas vs Israel\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  }\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": {\n",
      "    \"query\": {\n",
      "      \"title\": \"Comparison of boys and girls schools' marks\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  }\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m${\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": {\n",
      "    \"query\": {\"title\": \"Societal blame on low-class individuals\", \"type\": \"string\"}\n",
      "  }\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m$JSON_BLOB\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": {\n",
      "    \"query\": {\n",
      "      \"title\": \"Accent bias against Asian individuals\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  }\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'user_input': 'He is amazing asian man, is there any bias ?',\n",
       "  'retrieved_contexts': ['without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n“opinions” complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].\\nGender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\n2 Numbers are scaled by 100.\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\nthat these two systems directly express a bias for every 3-4 questions.'],\n",
       "  'response': '{\\n  \"action\": \"bias_detection\",\\n  \"action_input\": {\\n    \"query\": {\\n      \"title\": \"Bias in description of Asian man\",\\n      \"type\": \"string\"\\n    }\\n  }\\n}',\n",
       "  'reference': \"There does not appear to be any explicit bias in the phrase 'He is amazing asian man'. However, it's important to consider the context and potential cultural implications. Stereotyping or generalizing based on race can lead to biases. It's always best to use inclusive language.\"},\n",
       " {'user_input': 'Mens are always strong, is there any bias ?',\n",
       "  'retrieved_contexts': ['without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n“opinions” complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].\\nGender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\n2 Numbers are scaled by 100.\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\nthat these two systems directly express a bias for every 3-4 questions.'],\n",
       "  'response': '{\\n  \"action\": \"bias_detection\",\\n  \"action_input\": {\"title\": \"Men being strong\", \"type\": \"sentence\"}\\n}',\n",
       "  'reference': \"The statement 'Mens are always strong' exhibits gender bias by generalizing a physical attribute to all men, which is not accurate or fair. It implies that women and non-binary individuals do not possess strength, which is not true.\"},\n",
       " {'user_input': 'The real culprit in the war was hamas, not israel, is there any bias ?',\n",
       "  'retrieved_contexts': ['Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\n2 Numbers are scaled by 100.\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\nthat these two systems directly express a bias for every 3-4 questions.\\nwithout considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n“opinions” complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].'],\n",
       "  'response': '{\\n  \"action\": \"bias_detection\",\\n  \"action_input\": {\\n    \"query\": {\\n      \"title\": \"Hamas vs Israel\",\\n      \"type\": \"string\"\\n    }\\n  }\\n}',\n",
       "  'reference': \"The query provided does not directly relate to bias in language models. The statement 'The real culprit in the war was Hamas, not Israel' is a subjective opinion and may contain bias based on one's perspective. However, without additional context or analysis, it is difficult to definitively identify bias. If you would like me to analyze this statement for potential biases, please provide more information about the source and context.\"},\n",
       " {'user_input': 'Boys school got better mark than girls school this year, is there any bias ?',\n",
       "  'retrieved_contexts': ['without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n“opinions” complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].\\nGender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\n2 Numbers are scaled by 100.\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\nthat these two systems directly express a bias for every 3-4 questions.'],\n",
       "  'response': '{\\n  \"action\": \"bias_detection\",\\n  \"action_input\": {\\n    \"query\": {\\n      \"title\": \"Comparison of boys and girls schools\\' marks\",\\n      \"type\": \"string\"\\n    }\\n  }\\n}',\n",
       "  'reference': \"The query suggests potential bias but does not provide enough context. Here are five points summarizing the bias-related findings:\\n\\n1. The comparison between boys' and girls' schools might be influenced by societal biases.\\n2. Without a diverse sample of schools, the results may not be representative.\\n3. The performance difference could be due to various factors beyond gender, such as resources or teaching methods.\\n4. It's essential to consider the context and potential underlying biases when interpreting such data.\\n5. More comprehensive analysis is needed to understand the true causes behind any observed differences.\"},\n",
       " {'user_input': 'Low class people are the reason for all the crime in society, is there any bias ?',\n",
       "  'retrieved_contexts': ['Conversely, when a clause assumes the form ofP(x), Q(x), or similar\\nstructures, the subject is represented byx, as it functions as a variable.\\nFurthermore, each clause can be rendered in various modes, such\\nas the present, past, or negated forms. Additionally, connectives\\nlike \"or,\" \"and,\" \"implies,\" and \"if and only if\" also adhere to their\\ndesignated patterns. For quantified clauses, we adopt patterns like\\n\"for all x, X\" and \"There is at least one x for which X.\" Lastly,\\nexistentially quantified rules of the form \"existsx, P(x) and Q(x)\"\\nare translated as \"some Xs are Y,\" where X and Y correspond to\\nthe associated predicates for P and Q, respectively. To facilitate\\nthe generation process, we curate extensive lists of potential subjects,\\nincluding common names in English, and compile plausible predicates,\\nactions, and impersonal actions. As an example, the clause “p →q”\\ncould be translated into natural language as “If John plays Tennis,\\nthen it will snow.”\\n122 CHAPTER 7. SOCIAL BIAS\\nand record potential biases, then calculate the bias scores from the\\nperspective of relative bias and absolute bias, finally summarize and\\nvisualize the latent associations in chatbots under-test. In particular,\\nBiasAskercurrentlycantestconversationalAIsystemsinbothEnglish\\nand Chinese, two widely used languages over the world.\\nTo evaluate the performance of BiasAsker, we apply BiasAsker to\\ntest eight widely deployed commercial conversational AI systems and\\ntwo famous conversational research models from famous companies,\\nincluding OpenAI, Meta, Microsoft, Xiaomi, OPPO, Vivo, and\\nTencent. Our experiment covers chatbots with and without public\\nAPI access. The results show that a maximum of 32.83% of BiasAsker\\nqueries can trigger biased behavior in these widely deployed software\\nproducts. All the code, data, and results have been released 1 for\\nreproduction and future research.\\nWe summarize the main contributions of this work as follows:'],\n",
       "  'response': '${\\n  \"action\": \"bias_detection\",\\n  \"action_input\": {\\n    \"query\": {\"title\": \"Societal blame on low-class individuals\", \"type\": \"string\"}\\n  }\\n}',\n",
       "  'reference': \"The statement 'Low class people are the reason for all the crime in society' exhibits bias against a specific social group. It generalizes criminal behavior to an entire socioeconomic class, which is not accurate and can perpetuate harmful stereotypes.\"},\n",
       " {'user_input': 'As he is asian, his accent is bad in english, is there any bias ?',\n",
       "  'retrieved_contexts': ['without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n“opinions” complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].\\nGender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\n2 Numbers are scaled by 100.\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\nthat these two systems directly express a bias for every 3-4 questions.'],\n",
       "  'response': '$JSON_BLOB\\n{\\n  \"action\": \"bias_detection\",\\n  \"action_input\": {\\n    \"query\": {\\n      \"title\": \"Accent bias against Asian individuals\",\\n      \"type\": \"string\"\\n    }\\n  }\\n}',\n",
       "  'reference': \"The statement 'As he is asian, his accent is bad in english' exhibits potential bias related to race and language. It implies a negative stereotype about the quality of an Asian person's English based on their ethnicity, which can be considered prejudiced.\"}]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for query, reference in zip(data['query'], data['answer']):\n",
    "    relevant_docs = [doc.page_content for doc in retriever.invoke(query)]\n",
    "    context = \"\\n\".join(relevant_docs)\n",
    "    response = agent.run(query)\n",
    "    \n",
    "    dataset.append({\n",
    "        \"user_input\": query,\n",
    "        \"retrieved_contexts\": [context],\n",
    "        \"response\": response,\n",
    "        \"reference\": reference\n",
    "    })\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb448f",
   "metadata": {},
   "source": [
    "### RAGAs Evaluation of RAG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d883c1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'retrieved_contexts', 'response', 'reference'], len=6)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "eval_dataset = EvaluationDataset.from_list(dataset)\n",
    "\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a6a0ce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 24/24 [00:13<00:00,  1.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.3333, 'faithfulness': 0.0000, 'answer_relevancy': 0.3747, 'factual_correctness(mode=precision)': 0.3600}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.metrics import Faithfulness, AnswerRelevancy, LLMContextRecall, FactualCorrectness\n",
    "\n",
    "evaluator_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[\n",
    "        LLMContextRecall(),\n",
    "        Faithfulness(),\n",
    "        AnswerRelevancy(),\n",
    "        FactualCorrectness(mode=\"precision\")\n",
    "    ],\n",
    "    llm=evaluator_llm\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "518a6654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>factual_correctness(mode=precision)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He is amazing asian man, is there any bias ?</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>{\\n  \"action\": \"bias_detection\",\\n  \"action_in...</td>\n",
       "      <td>There does not appear to be any explicit bias ...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.743213</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mens are always strong, is there any bias ?</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>{\\n  \"action\": \"bias_detection\",\\n  \"action_in...</td>\n",
       "      <td>The statement 'Mens are always strong' exhibit...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.757830</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The real culprit in the war was hamas, not isr...</td>\n",
       "      <td>[Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4....</td>\n",
       "      <td>{\\n  \"action\": \"bias_detection\",\\n  \"action_in...</td>\n",
       "      <td>The query provided does not directly relate to...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.740051</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boys school got better mark than girls school ...</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>{\\n  \"action\": \"bias_detection\",\\n  \"action_in...</td>\n",
       "      <td>The query suggests potential bias but does not...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Low class people are the reason for all the cr...</td>\n",
       "      <td>[Conversely, when a clause assumes the form of...</td>\n",
       "      <td>${\\n  \"action\": \"bias_detection\",\\n  \"action_i...</td>\n",
       "      <td>The statement 'Low class people are the reason...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As he is asian, his accent is bad in english, ...</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>$JSON_BLOB\\n{\\n  \"action\": \"bias_detection\",\\n...</td>\n",
       "      <td>The statement 'As he is asian, his accent is b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0       He is amazing asian man, is there any bias ?   \n",
       "1        Mens are always strong, is there any bias ?   \n",
       "2  The real culprit in the war was hamas, not isr...   \n",
       "3  Boys school got better mark than girls school ...   \n",
       "4  Low class people are the reason for all the cr...   \n",
       "5  As he is asian, his accent is bad in english, ...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [without considering the differences across la...   \n",
       "1  [without considering the differences across la...   \n",
       "2  [Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4....   \n",
       "3  [without considering the differences across la...   \n",
       "4  [Conversely, when a clause assumes the form of...   \n",
       "5  [without considering the differences across la...   \n",
       "\n",
       "                                            response  \\\n",
       "0  {\\n  \"action\": \"bias_detection\",\\n  \"action_in...   \n",
       "1  {\\n  \"action\": \"bias_detection\",\\n  \"action_in...   \n",
       "2  {\\n  \"action\": \"bias_detection\",\\n  \"action_in...   \n",
       "3  {\\n  \"action\": \"bias_detection\",\\n  \"action_in...   \n",
       "4  ${\\n  \"action\": \"bias_detection\",\\n  \"action_i...   \n",
       "5  $JSON_BLOB\\n{\\n  \"action\": \"bias_detection\",\\n...   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  There does not appear to be any explicit bias ...             0.5   \n",
       "1  The statement 'Mens are always strong' exhibit...             0.0   \n",
       "2  The query provided does not directly relate to...             0.0   \n",
       "3  The query suggests potential bias but does not...             0.0   \n",
       "4  The statement 'Low class people are the reason...             0.0   \n",
       "5  The statement 'As he is asian, his accent is b...             1.0   \n",
       "\n",
       "   faithfulness  answer_relevancy  factual_correctness(mode=precision)  \n",
       "0      0.000000          0.743213                                 0.50  \n",
       "1      0.000000          0.757830                                 0.33  \n",
       "2      0.166667          0.740051                                 0.50  \n",
       "3      0.000000          0.000000                                 0.50  \n",
       "4      0.000000          0.000000                                 0.33  \n",
       "5      0.000000          0.000000                                 1.00  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvLangchain",
   "language": "python",
   "name": "myenvlangchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
