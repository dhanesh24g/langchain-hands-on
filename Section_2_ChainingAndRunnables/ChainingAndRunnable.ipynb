{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c8b6c3",
   "metadata": {},
   "source": [
    "## Understanding Chaining And Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc6d9f",
   "metadata": {},
   "source": [
    "### Load the ENV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf50107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyFirstLangchainAutomation\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "load = load_dotenv('../.env')\n",
    "\n",
    "print(os.getenv(\"LANGSMITH_PROJECT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91434a",
   "metadata": {},
   "source": [
    "### Create LLM Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a15f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import chain\n",
    "import re\n",
    "\n",
    "# ChatOllama (LLM) is a RunnableInterface itself\n",
    "llm_qwen = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# ChatOllama (LLM) is a RunnableInterface itself\n",
    "llm_deepseek = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"deepseek-r1:8b\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# Making remove_think_block as a Runnable in order to chain it\n",
    "@chain\n",
    "def remove_think_block(text) -> str:\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994fd71",
   "metadata": {},
   "source": [
    "### Chaining the Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5dc617e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\nRunning AI models on the **cloud** offers numerous advantages, especially when compared to running them on-premises or using local infrastructure. Here are the key benefits of running AI models on the **cloud**:\\n\\n---\\n\\n### 1. **Scalability**\\n- **On-demand resources**: Cloud platforms allow you to scale compute, storage, and memory resources up or down as needed.\\n- **Handling large workloads**: AI models, especially those involving deep learning, require significant computational power, which the cloud can provide dynamically.\\n\\n---\\n\\n### 2. **Cost Efficiency**\\n- **Pay-as-you-go model**: You only pay for the resources you use, which reduces upfront capital expenditure.\\n- **No need for expensive hardware**: Cloud providers offer GPU and TPU instances that are optimized for AI workloads, eliminating the need for expensive on-premises hardware.\\n\\n---\\n\\n### 3. **Access to Advanced AI Tools and Services**\\n- **Pre-built AI/ML services**: Cloud platforms (e.g., AWS SageMaker, Azure Machine Learning, Google AI Platform) provide ready-to-use tools for model training, deployment, and monitoring.\\n- **Integration with other services**: Cloud platforms offer seamless integration with data storage, databases, APIs, and analytics tools, streamlining the AI workflow.\\n\\n---\\n\\n### 4. **High Availability and Reliability**\\n- **Redundancy and failover**: Cloud providers offer built-in redundancy and failover mechanisms to ensure high availability and reliability of AI models.\\n- **Global reach**: AI models can be deployed in multiple regions, enabling low-latency access for users around the world.\\n\\n---\\n\\n### 5. **Ease of Deployment and Management**\\n- **Automated scaling and load balancing**: Cloud platforms automatically handle scaling and load balancing, ensuring optimal performance.\\n- **Model versioning and monitoring**: Cloud services provide tools to track model performance, monitor for drift, and manage model versions.\\n\\n---\\n\\n### 6. **Collaboration and Team Access**\\n- **Shared access**: Cloud environments allow multiple team members to access and collaborate on AI models and data from anywhere.\\n- **Version control and CI/CD**: Cloud platforms support version control and continuous integration/continuous deployment (CI/CD) pipelines for AI models.\\n\\n---\\n\\n### 7. **Security and Compliance**\\n- **Built-in security features**: Cloud providers offer encryption, access controls, and audit trails to secure AI models and data.\\n- **Compliance with regulations**: Cloud platforms often comply with industry standards and regulations (e.g., GDPR, HIPAA), which is crucial for sensitive data.\\n\\n---\\n\\n### 8. **Support for Hybrid and Multi-Cloud Environments**\\n- **Flexibility in deployment**: AI models can be deployed in hybrid or multi-cloud environments, allowing organizations to leverage the best of on-premises and cloud infrastructure.\\n- **Seamless integration with on-premises systems**: Cloud platforms often support integration with existing data centers and legacy systems.\\n\\n---\\n\\n### 9. **Rapid Prototyping and Experimentation**\\n- **Quick setup**: Cloud environments allow for rapid setup of AI development environments, enabling faster prototyping and experimentation.\\n- **Access to large datasets**: Cloud platforms often provide access to large, curated datasets for training and testing AI models.\\n\\n---\\n\\n### 10. **Support for Edge AI and IoT Integration**\\n- **Edge computing integration**: Cloud platforms support edge computing, allowing AI models to run on edge devices while still being managed and monitored from the cloud.\\n- **IoT data processing**: Cloud platforms can process and analyze data from IoT devices in real-time, enabling smart decision-making.\\n\\n---\\n\\n### Summary\\nRunning AI models on the cloud provides **scalability, cost efficiency, advanced tools, reliability, ease of management, security, and flexibility**. It allows organizations to focus on innovation rather than infrastructure, making it an ideal choice for AI development and deployment.', additional_kwargs={}, response_metadata={'model': 'qwen3:latest', 'created_at': '2025-06-05T00:15:31.114333Z', 'done': True, 'done_reason': 'stop', 'total_duration': 33291201792, 'load_duration': 3539839292, 'prompt_eval_count': 34, 'prompt_eval_duration': 1258409791, 'eval_count': 784, 'eval_duration': 28490667875, 'model_name': 'qwen3:latest'}, id='run--e720a732-f727-490a-b9e0-596490c446bb-0', usage_metadata={'input_tokens': 34, 'output_tokens': 784, 'total_tokens': 818})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"), \n",
    "    (\"user\", \"What is the advantage of {action} AI models on Cloud ? /no_think\")\n",
    "    ])\n",
    "\n",
    "# Chaining the prompt & LLM by providing the input of prompt to the LLM itself\n",
    "chain = prompt | llm_qwen\n",
    "\n",
    "chain.invoke({\"action\": \"running\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba7c2e",
   "metadata": {},
   "source": [
    "### Output STRING Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ?\"),\n",
    "])\n",
    "\n",
    "# Chaining the outputParser along with the prompt & llm\n",
    "chain = prompt | llm_deepseek | StrOutputParser()\n",
    "\n",
    "output = chain.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed72bf",
   "metadata": {},
   "source": [
    "### Chaining Multiple Chains | RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f512b0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's break this down. The user wants a list of headings extracted from the assistant's response about running LLMs locally versus cloud-based.\n",
      "\n",
      "First, I'll look at the structure provided by the assistant. It starts with an introduction section, then lists eight advantages under \"Running a Large Language Model (LLM) on a local machine...\" Each advantage has bullet points and explanations.\n",
      "\n",
      "Next is the \"When to Consider Local Deployment?\" subsection followed by criteria like data privacy needs. Then there's a parallel part for cloud deployment considerations. Finally, there's a summary table comparing aspects between local and cloud options.\n",
      "\n",
      "I need to make sure I capture all main sections including:\n",
      "- The introduction paragraph\n",
      "- Each numbered advantage with its explanation\n",
      "- The subheadings \"When to Consider Local Deployment?\" and related criteria\n",
      "- Similarly structured parts for Cloud-Based LLMs\n",
      "- The summary section which includes the table\n",
      "\n",
      "The user probably wants these headings to quickly reference key points or perhaps to structure their own document. They might be looking for a concise outline without diving into detailed explanations.\n",
      "\n",
      "I should verify that all sections are properly identified and that any nested subsections (like the compliance point within advantages) aren't mistaken as main headings unless they're explicitly titled. The table at the end is also a section heading, so it needs to be included.\n",
      "</think>\n",
      "Here are the headings from the response:\n",
      "\n",
      "*   Running a Large Language Model (LLM) on a local machine offers several advantages...\n",
      "    *   Data Privacy and Security\n",
      "        *   No data transmission to the cloud\n",
      "        *   Compliance with regulations\n",
      "    *   Offline Access\n",
      "        *   No internet connection required\n",
      "        - Reliable operation\n",
      "    *   Customization and Control\n",
      "        *   Fine-tuning and personalization\n",
      "        *   Model optimization\n",
      "    *   Cost Efficiency (in some cases)\n",
      "        *   Reduced cloud costs\n",
      "        *   Avoiding API rate limits\n",
      "    *   Performance and Latency\n",
      "        *   Lower latency\n",
      "        *   Better control over performance\n",
      "    *   Scalability (in some contexts)\n",
      "        *   Distributed inference\n",
      "        *   Edge computing\n",
      "    *   Avoiding Dependency on Third Parties\n",
      "        *   No reliance on cloud providers\n",
      "        - Ownership of the model\n",
      "*   When to Consider Local Deployment?\n",
      "    *   Data privacy or compliance needs\n",
      "    *   Offline operation requirements\n",
      "    *   Customization or fine-tuning goals\n",
      "    *   Restricted network environments\n",
      "    *   Low-latency inference demands\n",
      "*   When to Use Cloud-Based LLMs?\n",
      "    *   Scalable infrastructure needed\n",
      "    *   Pre-trained models sufficient without tuning\n",
      "    *   Ease of use and rapid deployment desired\n",
      "    *   Limited local hardware resources (CPU, GPU memory etc.)\n",
      "*   Summary\n",
      "    *   Aspect Comparison Table\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "firstPrompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ? /no_think\"),\n",
    "])\n",
    "\n",
    "detailedResponseChain = firstPrompt | llm_qwen | StrOutputParser()\n",
    "\n",
    "followingPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                     Analyse the response & get me just the headings from the {response}.\n",
    "                                     Response should be in bullet points. Please do not provide explanation, but only precise output.\n",
    "                                     \"\"\")\n",
    "\n",
    "chainWithHeading = {\"response\": detailedResponseChain} | followingPrompt | llm_deepseek | StrOutputParser()\n",
    "\n",
    "output = chainWithHeading.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907ea1b",
   "metadata": {},
   "source": [
    "### Running Chains in Parallel | RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02c8c984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Running a Large Language Model (LLM) on a local machine offers several advantages, depending on the use case, environment, and requirements. Here are the key benefits:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Privacy and Data Security**\n",
      "- **Data Confidentiality**: Sensitive data (e.g., personal information, business secrets) remains on the local machine and is not transmitted to external servers.\n",
      "- **Compliance**: Helps meet regulatory and compliance requirements (e.g., GDPR, HIPAA) by keeping data within the organization's network.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Control Over the Environment**\n",
      "- **Customization**: You have full control over the model’s training data, fine-tuning, and deployment environment.\n",
      "- **Customization of Inference**: You can tailor the model for specific tasks (e.g., code generation, translation, or domain-specific tasks) without relying on cloud APIs.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Offline Access**\n",
      "- **No Internet Dependency**: The model can operate without an internet connection, making it ideal for remote locations, field operations, or environments with restricted network access.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Cost Efficiency (in Some Cases)**\n",
      "- **Reduced API Costs**: If you're using a cloud-based LLM API (like GPT-4 or PaLM), running the model locally can reduce costs, especially for high-volume usage.\n",
      "- **Long-Term Cost Savings**: Once set up, running an LLM locally can be more cost-effective than paying for cloud API usage over time.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. **Performance and Latency**\n",
      "- **Lower Latency**: Running the model locally reduces network latency, which is critical for real-time applications like chatbots, live support, or interactive tools.\n",
      "- **Predictable Performance**: You can optimize hardware and software configurations for consistent and reliable performance.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. **Scalability (With Proper Infrastructure)**\n",
      "- **Custom Hardware Optimization**: You can choose the right hardware (e.g., GPUs, TPUs, or specialized AI chips) to maximize performance and efficiency.\n",
      "- **Distributed Inference**: For large-scale deployments, you can set up distributed computing environments to scale the model’s capabilities.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. **Avoiding API Rate Limits**\n",
      "- **Unlimited Usage**: You are not subject to the rate limits of cloud APIs, which can be a constraint in high-throughput applications.\n",
      "\n",
      "---\n",
      "\n",
      "### 8. **Integration with On-Premise Systems**\n",
      "- **Seamless Integration**: The model can be integrated directly with internal systems, databases, and workflows without requiring external API calls.\n",
      "\n",
      "---\n",
      "\n",
      "### 9. **Research and Development Flexibility**\n",
      "- **Experimentation**: Researchers and developers can experiment with model architectures, training data, and fine-tuning techniques in a controlled environment.\n",
      "- **Reproducibility**: Local setups allow for more reproducible and repeatable experiments.\n",
      "\n",
      "---\n",
      "\n",
      "### 10. **Reduced Reliance on Cloud Providers**\n",
      "- **Vendor Lock-In Avoidance**: By running the model locally, you reduce dependency on specific cloud providers, which can be beneficial in terms of cost and flexibility.\n",
      "\n",
      "---\n",
      "\n",
      "### Summary Table:\n",
      "\n",
      "| Advantage | Description |\n",
      "|-----------|-------------|\n",
      "| Privacy & Security | Sensitive data stays local, reducing exposure to breaches. |\n",
      "| Control & Customization | Full control over model configuration and deployment. |\n",
      "| Offline Operation | No internet dependency for model execution. |\n",
      "| Cost Efficiency | Lower costs for high-volume or long-term use. |\n",
      "| Low Latency | Faster response times due to local execution. |\n",
      "| Scalability | Can be scaled with custom hardware and infrastructure. |\n",
      "| Avoid API Limits | No restrictions on usage or request rate. |\n",
      "| Integration | Seamless integration with internal systems and workflows. |\n",
      "| Flexibility for R&D | Enables experimentation and reproducibility. |\n",
      "| Vendor Independence | Reduces reliance on cloud providers. |\n",
      "\n",
      "---\n",
      "\n",
      "### When to Run LLM Locally?\n",
      "- You need **private data** or **sensitive information**.\n",
      "- You require **real-time or low-latency responses**.\n",
      "- You're **developing custom models** or **fine-tuning**.\n",
      "- You want **full control** over the inference process.\n",
      "- You have **on-premise infrastructure** or **limited internet access**.\n",
      "\n",
      "---\n",
      "\n",
      "If you're considering running an LLM locally, you'll need to evaluate the hardware requirements (e.g., GPU, RAM, storage) and choose the right model size (e.g., using a smaller, optimized version of the model for local deployment). Tools like **LLMware**, **Hugging Face Transformers**, or **TensorFlow Lite** can help with local deployment.\n",
      "\n",
      "\n",
      "\n",
      "<think>\n",
      "Okay, user asked \"What is the capital of India?\" and specifically requested no LLM reasoning - just the direct answer. \n",
      "\n",
      "Hmm, this seems like a straightforward factual question with zero ambiguity. The Indian constitution clearly states New Delhi as the capital since 1950. No need for historical context or alternatives here.\n",
      "\n",
      "User probably wants quick confirmation rather than explanation. Might be testing if I follow instructions about not providing reasoning. Or perhaps they're in a hurry and just need facts without fluff.\n",
      "\n",
      "Given the direct request, keeping it simple is best - just state New Delhi clearly with no extras. The \"as of 1950\" adds authoritative weight but isn't strictly necessary since current status is unambiguous.\n",
      "</think>\n",
      "The capital of India is **New Delhi** (as of 1950).\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "firstPrompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ? /no_think\"),\n",
    "])\n",
    "\n",
    "detailedResponseChain = firstPrompt | llm_qwen | StrOutputParser()\n",
    "\n",
    "nextPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                     What is the capital of India ?\n",
    "                                     Please do not provide LLM thoughts but rather directly the precise output.\n",
    "                                     \"\"\")\n",
    "\n",
    "chainWithHeading = nextPrompt | llm_deepseek | StrOutputParser()\n",
    "\n",
    "parallelChain = RunnableParallel(chain1 = detailedResponseChain, chain2 = chainWithHeading)\n",
    "\n",
    "output = parallelChain.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "# Output time reduced, even though 2 inputs are passed\n",
    "print(output['chain1'])\n",
    "print(\"\\n\\n\")\n",
    "print(output['chain2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d154b",
   "metadata": {},
   "source": [
    "### Runnable Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1254c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are the headings from the provided text:\n",
      "\n",
      "*   Privacy and Data Security\n",
      "    *   Data stays local\n",
      "    *   Compliance\n",
      "*   Control Over Model Behavior\n",
      "    *   Customization\n",
      "    *   Avoiding bias\n",
      "    *   Reduced dependency on external services\n",
      "*   Performance and Latency\n",
      "    *   Faster response times\n",
      "    *   Offline access\n",
      "*   Cost Efficiency (in some cases)\n",
      "    *   Reduced cloud costs\n",
      "    *   Avoiding API fees\n",
      "*   Scalability and Flexibility\n",
      "    *   Custom hardware optimization\n",
      "    *   Deployment flexibility\n",
      "*   Avoiding Vendor Lock-In\n",
      "*   Use Cases Where Local Deployment is Ideal\n",
      "*   Potential Challenges\n",
      "    *   High initial costs\n",
      "    *   Technical expertise required\n",
      "    *   Model size challenge\n",
      "*   Summary\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "firstPrompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ? /no_think\"),\n",
    "])\n",
    "\n",
    "# Chain 1\n",
    "firstResponseChain = firstPrompt | llm_qwen | StrOutputParser()\n",
    "\n",
    "followingPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                     Analyse the response & get me just the headings from the {response}.\n",
    "                                     Response should be in bullet points. Without explanation, but just the headings.\n",
    "                                     \"\"\")\n",
    "\n",
    "# Creating the RunnableLamda\n",
    "decide_llm = lambda response: llm_deepseek if len(str(response)) > 300 else llm_qwen\n",
    "\n",
    "llm_selector = RunnableLambda(decide_llm)\n",
    "\n",
    "#Chain 2\n",
    "chainWithHeading = {\"response\": firstResponseChain} | followingPrompt | llm_selector | StrOutputParser() | remove_think_block\n",
    "\n",
    "output = chainWithHeading.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585059cd",
   "metadata": {},
   "source": [
    "### Using @chain Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e6bc226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are the main headings from your response:\n",
      "\n",
      "- **Privacy and Data Security**\n",
      "- **Control Over the Model**\n",
      "- **Offline Access**\n",
      "- **Performance and Latency**\n",
      "- **Cost Efficiency (in some cases)**\n",
      "- **Custom Deployment**\n",
      "- **Avoid Vendor Lock-in**\n",
      "\n",
      "Let me know if you need help extracting specific ones!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "firstPrompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ? /no_think\"),\n",
    "])\n",
    "\n",
    "# Chain 1\n",
    "firstResponseChain = firstPrompt | llm_qwen | StrOutputParser()\n",
    "\n",
    "followingPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                     Analyse the response & get me just the headings from the {response}.\n",
    "                                     Response should be in bullet points.\n",
    "                                     \"\"\")\n",
    "\n",
    "# Using the @chain decorator to make `decide_llm` Runnable\n",
    "@chain\n",
    "def decide_llm(response):\n",
    "    response_text = str(response)\n",
    "    if len(response_text) > 300:\n",
    "        return llm_deepseek\n",
    "    return llm_qwen\n",
    "\n",
    "#Chain 2\n",
    "chainWithHeading = {\"response\": firstResponseChain} | followingPrompt | decide_llm | StrOutputParser() | remove_think_block\n",
    "\n",
    "output = chainWithHeading.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvLangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
