{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c8b6c3",
   "metadata": {},
   "source": [
    "## Understanding Chaining And Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc6d9f",
   "metadata": {},
   "source": [
    "### Load the ENV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf50107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyFirstLangchainAutomation\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "load = load_dotenv('../.env')\n",
    "\n",
    "print(os.getenv(\"LANGSMITH_PROJECT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91434a",
   "metadata": {},
   "source": [
    "### Create LLM Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.messages import AIMessage\n",
    "from typing import Union\n",
    "import re\n",
    "\n",
    "# ChatOllama (LLM) is a RunnableInterface itself\n",
    "llm_qwen = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# ChatOllama (LLM) is a RunnableInterface itself\n",
    "llm_llama = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# Making remove_any_think_block as a Runnable in order to chain it\n",
    "@chain\n",
    "def remove_any_think_block(input: Union[str, AIMessage]) -> str:\n",
    "    if isinstance (input, AIMessage):\n",
    "        input = input.content\n",
    "    \n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", input, flags=re.DOTALL | re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994fd71",
   "metadata": {},
   "source": [
    "### Chaining the Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5dc617e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\nRunning AI models on the **cloud** offers numerous advantages, making it a preferred choice for developers, data scientists, and organizations. Here are the key benefits of running AI models on the cloud:\\n\\n---\\n\\n### 1. **Scalability**\\n- **On-demand resources**: Cloud platforms provide the ability to scale computing power, storage, and memory up or down based on the workload.\\n- **Elasticity**: AI models can be scaled to handle large volumes of data or increased inference requests without the need for physical hardware upgrades.\\n\\n---\\n\\n### 2. **Cost Efficiency**\\n- **Pay-as-you-go model**: You only pay for the resources you actually use, which reduces upfront capital expenditure.\\n- **No need for on-premises infrastructure**: Avoid the costs of maintaining physical servers, cooling, and other infrastructure.\\n\\n---\\n\\n### 3. **Access to Advanced Tools and Services**\\n- **AI/ML platforms**: Cloud providers offer integrated AI/ML platforms (e.g., AWS SageMaker, Azure Machine Learning, Google AI Platform) with built-in tools for model training, deployment, and monitoring.\\n- **Pre-built AI models**: Access to pre-trained models (e.g., BERT, ResNet, etc.) and model marketplaces can accelerate development.\\n\\n---\\n\\n### 4. **High Availability and Reliability**\\n- **Redundancy**: Cloud providers ensure high availability with redundant systems and global data centers.\\n- **Disaster recovery**: Cloud services offer built-in backup and disaster recovery options to ensure model availability.\\n\\n---\\n\\n### 5. **Collaboration and Team Work**\\n- **Centralized platforms**: Cloud-based collaboration tools allow teams to work on the same models, datasets, and environments from anywhere.\\n- **Version control and CI/CD**: Integration with DevOps tools enables model versioning, testing, and continuous deployment.\\n\\n---\\n\\n### 6. **Ease of Deployment and Management**\\n- **Serverless computing**: Services like AWS Lambda or Azure Functions allow deploying AI models without managing servers.\\n- **Model serving**: Cloud platforms offer managed model serving (e.g., AWS SageMaker Hosting, Azure Model Management) to deploy models quickly and efficiently.\\n\\n---\\n\\n### 7. **Security and Compliance**\\n- **Built-in security**: Cloud providers offer encryption, IAM (Identity and Access Management), and compliance certifications (e.g., GDPR, HIPAA).\\n- **Data privacy**: Cloud services allow secure data handling and access control, which is critical for sensitive AI applications.\\n\\n---\\n\\n### 8. **Global Reach and Latency Optimization**\\n- **Global data centers**: Cloud services enable deployment of AI models in regions close to end-users, reducing latency and improving performance.\\n- **Edge computing integration**: Cloud providers support edge AI by integrating with edge devices and IoT platforms.\\n\\n---\\n\\n### 9. **Support for Large-Scale Data Processing**\\n- **Big data capabilities**: Cloud platforms offer tools for processing and analyzing large datasets (e.g., Spark, Hadoop, BigQuery).\\n- **Distributed computing**: AI models can be trained on distributed clusters, enabling faster and more efficient training.\\n\\n---\\n\\n### 10. **Innovation and Future-Proofing**\\n- **Access to cutting-edge technologies**: Cloud providers regularly update their AI/ML tools, frameworks, and hardware (e.g., GPUs, TPUs) to keep up with the latest advancements.\\n- **Future scalability**: Cloud infrastructure is more adaptable to future AI innovations and evolving model requirements.\\n\\n---\\n\\n### Summary\\n\\nRunning AI models on the **cloud** provides a flexible, cost-effective, and powerful way to develop, train, deploy, and manage AI systems. It enables organizations to focus on innovation and business outcomes rather than infrastructure management, making it a cornerstone of modern AI deployment.', additional_kwargs={}, response_metadata={'model': 'qwen3:latest', 'created_at': '2025-07-19T10:03:08.466627Z', 'done': True, 'done_reason': 'stop', 'total_duration': 29291495291, 'load_duration': 52800416, 'prompt_eval_count': 34, 'prompt_eval_duration': 1334704292, 'eval_count': 755, 'eval_duration': 27894979125, 'model_name': 'qwen3:latest'}, id='run--2e743972-6972-4e92-a917-89d458905a90-0', usage_metadata={'input_tokens': 34, 'output_tokens': 755, 'total_tokens': 789})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"), \n",
    "    (\"user\", \"What is the advantage of {action} AI models on Cloud ? /no_think\")\n",
    "    ])\n",
    "\n",
    "# Chaining the prompt & LLM by providing the input of prompt to the LLM itself\n",
    "chain = prompt | llm_qwen\n",
    "\n",
    "chain.invoke({\"action\": \"running\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba7c2e",
   "metadata": {},
   "source": [
    "### Output STRING Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eda4f5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running a Large Language Model (LLM) on a **local machine** offers several advantages, depending on the use case, infrastructure, and requirements. Hereâ€™s a structured breakdown of the key benefits:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Data Privacy and Security**\n",
      "- **Sensitivity of Data**: When handling **sensitive or proprietary data** (e.g., healthcare records, financial information, or intellectual property), running the model locally avoids transmitting data over the internet, reducing the risk of data breaches or unauthorized access.\n",
      "- **Compliance**: Satisfies **data localization laws** (e.g., GDPR in the EU, CCPA in the US) that require sensitive data to be processed within specific geographic boundaries.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Control and Customization**\n",
      "- **Fine-Tuning and Optimization**: Local deployment allows **hyperparameter tuning**, **model quantization**, or **custom training** to adapt the LLM to specific tasks (e.g., domain-specific language, niche applications).\n",
      "- **Avoiding Vendor Lock-in**: Reduces dependency on cloud providers, enabling **in-house customization** and **proprietary integration** with internal systems.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Cost Efficiency**\n",
      "- **Reduced Cloud Costs**: While cloud providers offer scalable resources, running an LLM locally can be **more cost-effective** for frequent or long-term usage, especially with optimized hardware (e.g., GPUs/TPUs).\n",
      "- **Avoiding API Fees**: Eliminates recurring costs for cloud API calls or model inference services.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Offline Capabilities**\n",
      "- **No Internet Dependency**: Critical for **remote or disconnected environments** (e.g., field operations, military, or rural areas) where internet connectivity is unreliable or unavailable.\n",
      "- **Real-Time Processing**: Ensures **low-latency responses** for time-sensitive applications (e.g., live chatbots, voice assistants, or industrial automation).\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Performance and Latency**\n",
      "- **Faster Inference**: Local execution minimizes **network latency**, improving response times for applications like real-time translation, customer service, or interactive AI tools.\n",
      "- **Resource Allocation**: Direct control over hardware (e.g., GPUs/TPUs) allows **optimized resource allocation** for specific workloads.\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Energy Efficiency and Sustainability**\n",
      "- **Localized Infrastructure**: Running models on-premises can be more **energy-efficient** if the infrastructure is optimized (e.g., using renewable energy sources or energy-efficient hardware).\n",
      "- **Carbon Footprint Reduction**: Avoids reliance on data centers, which often have significant environmental impacts.\n",
      "\n",
      "---\n",
      "\n",
      "### **7. Compliance and Regulatory Requirements**\n",
      "- **Regulatory Adherence**: Ensures compliance with **industry-specific regulations** (e.g., HIPAA for healthcare, PCI-DSS for finance) that mandate data processing within secure, controlled environments.\n",
      "\n",
      "---\n",
      "\n",
      "### **8. Integration with Internal Systems**\n",
      "- **Seamless Integration**: Easier to integrate the LLM with **existing on-premises systems**, databases, or workflows without relying on external APIs.\n",
      "- **Custom Workflows**: Tailor the model to fit **internal processes** (e.g., document analysis, code generation, or enterprise search).\n",
      "\n",
      "---\n",
      "\n",
      "### **Considerations and Trade-offs**\n",
      "- **Hardware Requirements**: Requires **high-performance hardware** (e.g., GPUs/TPUs) to run large models, which can be expensive to acquire and maintain.\n",
      "- **Scalability**: May be less scalable than cloud solutions for **massive workloads** or **global user bases**.\n",
      "- **Maintenance**: Involves **ongoing maintenance**, updates, and monitoring of local infrastructure.\n",
      "\n",
      "---\n",
      "\n",
      "### **Use Cases Where Local Deployment is Ideal**\n",
      "- **Enterprise Use**: For organizations with sensitive data or compliance needs.\n",
      "- **Offline Applications**: Remote operations, IoT devices, or field work.\n",
      "- **Customization Needs**: Tailored models for specific industries or tasks.\n",
      "- **Cost-Sensitive Environments**: Long-term or frequent model usage where local costs outweigh cloud expenses.\n",
      "\n",
      "---\n",
      "\n",
      "In summary, running an LLM locally is ideal for **privacy-sensitive, compliance-driven, or resource-intensive applications**, while cloud deployment is often better for **scalability, ease of use, or global accessibility**. The choice depends on the specific priorities of the use case.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ?\"),\n",
    "])\n",
    "\n",
    "# Chaining the outputParser along with the prompt & llm\n",
    "# chain = prompt | llm_qwen | StrOutputParser()\n",
    "chain = prompt | llm_qwen | remove_any_think_block\n",
    "\n",
    "output = chain.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed72bf",
   "metadata": {},
   "source": [
    "### Chaining Multiple Chains | RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "firstPrompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ? /no_think\"),\n",
    "])\n",
    "\n",
    "detailedResponseChain = firstPrompt | llm_qwen | StrOutputParser()\n",
    "\n",
    "followingPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                     Analyse the response & get me just the headings from the {response}.\n",
    "                                     Response should be in bullet points. Please do not provide explanation, but only precise output.\n",
    "                                     \"\"\")\n",
    "\n",
    "chainWithHeading = {\"response\": detailedResponseChain} | followingPrompt | llm_llama | StrOutputParser()\n",
    "\n",
    "output = chainWithHeading.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907ea1b",
   "metadata": {},
   "source": [
    "### Running Chains in Parallel | RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "firstPrompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ? /no_think\"),\n",
    "])\n",
    "\n",
    "detailedResponseChain = firstPrompt | llm_qwen | StrOutputParser()\n",
    "\n",
    "nextPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                     What is the capital of India ?\n",
    "                                     Please do not provide LLM thoughts but rather directly the precise output.\n",
    "                                     \"\"\")\n",
    "\n",
    "chainWithHeading = nextPrompt | llm_llama | StrOutputParser()\n",
    "\n",
    "parallelChain = RunnableParallel(chain1 = detailedResponseChain, chain2 = chainWithHeading)\n",
    "\n",
    "output = parallelChain.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "# Output time reduced, even though 2 inputs are passed\n",
    "print(output['chain1'])\n",
    "print(\"\\n\\n\")\n",
    "print(output['chain2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d154b",
   "metadata": {},
   "source": [
    "### Runnable Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1254c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¢ **Privacy and Data Security**\n",
      "â€¢ **Customization and Flexibility**\n",
      "â€¢ **Performance and Latency**\n",
      "â€¢ **Cost Efficiency**\n",
      "â€¢ **Avoiding Vendor Lock-in**\n",
      "â€¢ **Regulatory Compliance**\n",
      "â€¢ **Scalability and Integration**\n",
      "â€¢ **Training and Development** \n",
      "â€¢ **Summary**\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "firstPrompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ? /no_think\"),\n",
    "])\n",
    "\n",
    "# Chain 1\n",
    "firstResponseChain = firstPrompt | llm_qwen | StrOutputParser()\n",
    "\n",
    "followingPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                     Analyse the response & get me just the headings from the {response}.\n",
    "                                     Response should be in bullet points. Without explanation, but just the headings.\n",
    "                                     \"\"\")\n",
    "\n",
    "# Creating the RunnableLamda\n",
    "decide_llm = lambda response: llm_llama if len(str(response)) > 300 else llm_qwen\n",
    "\n",
    "llm_selector = RunnableLambda(decide_llm)\n",
    "\n",
    "#Chain 2\n",
    "chainWithHeading = {\"response\": firstResponseChain} | followingPrompt | llm_selector | StrOutputParser() | remove_any_think_block\n",
    "\n",
    "output = chainWithHeading.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585059cd",
   "metadata": {},
   "source": [
    "### Using @chain Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6bc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "firstPrompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ? /no_think\"),\n",
    "])\n",
    "\n",
    "# Chain 1\n",
    "firstResponseChain = firstPrompt | llm_qwen | StrOutputParser()\n",
    "\n",
    "followingPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                     Analyse the response & get me just the headings from the {response}.\n",
    "                                     Response should be in bullet points.\n",
    "                                     \"\"\")\n",
    "\n",
    "# Using the @chain decorator to make `decide_llm` Runnable\n",
    "@chain\n",
    "def decide_llm(response):\n",
    "    response_text = str(response)\n",
    "    if len(response_text) > 300:\n",
    "        return llm_llama\n",
    "    return llm_qwen\n",
    "\n",
    "#Chain 2\n",
    "chainWithHeading = {\"response\": firstResponseChain} | followingPrompt | decide_llm | remove_any_think_block\n",
    "\n",
    "output = chainWithHeading.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvLangchain",
   "language": "python",
   "name": "myenvlangchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
