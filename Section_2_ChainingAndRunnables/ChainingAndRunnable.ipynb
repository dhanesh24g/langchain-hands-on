{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c8b6c3",
   "metadata": {},
   "source": [
    "## Understanding Chaining And Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc6d9f",
   "metadata": {},
   "source": [
    "### Load the ENV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf50107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "load = load_dotenv('../.env')\n",
    "\n",
    "print(os.getenv(\"LANGSMITH_PROJECT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91434a",
   "metadata": {},
   "source": [
    "### Create LLM Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.messages import AIMessage\n",
    "from typing import Union\n",
    "import re\n",
    "\n",
    "# ChatOllama (LLM) is a RunnableInterface itself\n",
    "llm_qwen = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# ChatOllama (LLM) is a RunnableInterface itself\n",
    "llm_llama = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# Making remove_any_think_block as a Runnable in order to chain it\n",
    "@chain\n",
    "def remove_any_think_block(input: Union[str, AIMessage]) -> str:\n",
    "    if isinstance (input, AIMessage):\n",
    "        input = input.content\n",
    "    \n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", input, flags=re.DOTALL | re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994fd71",
   "metadata": {},
   "source": [
    "### Chaining the Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"), \n",
    "    (\"user\", \"What is the advantage of {action} AI models on Cloud ? /no_think\")\n",
    "    ])\n",
    "\n",
    "# Chaining the prompt & LLM by providing the input of prompt to the LLM itself\n",
    "chain = prompt | llm_qwen\n",
    "\n",
    "chain.invoke({\"action\": \"running\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba7c2e",
   "metadata": {},
   "source": [
    "### Output STRING Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ?\"),\n",
    "])\n",
    "\n",
    "# Chaining the outputParser along with the prompt & llm\n",
    "# chain = prompt | llm_qwen | StrOutputParser()\n",
    "chain = prompt | llm_qwen | remove_any_think_block\n",
    "\n",
    "output = chain.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed72bf",
   "metadata": {},
   "source": [
    "### Chaining Multiple Chains | RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "firstPrompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ? /no_think\"),\n",
    "])\n",
    "\n",
    "detailedResponseChain = firstPrompt | llm_qwen | StrOutputParser()\n",
    "\n",
    "followingPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                     Analyse the response & get me just the headings from the {response}.\n",
    "                                     Response should be in bullet points. Please do not provide explanation, but only precise output.\n",
    "                                     \"\"\")\n",
    "\n",
    "chainWithHeading = {\"response\": detailedResponseChain} | followingPrompt | llm_llama | StrOutputParser()\n",
    "\n",
    "output = chainWithHeading.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907ea1b",
   "metadata": {},
   "source": [
    "### Running Chains in Parallel | RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "firstPrompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ? /no_think\"),\n",
    "])\n",
    "\n",
    "detailedResponseChain = firstPrompt | llm_qwen | StrOutputParser()\n",
    "\n",
    "nextPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                     What is the capital of India ?\n",
    "                                     Please do not provide LLM thoughts but rather directly the precise output.\n",
    "                                     \"\"\")\n",
    "\n",
    "chainWithHeading = nextPrompt | llm_llama | StrOutputParser()\n",
    "\n",
    "parallelChain = RunnableParallel(chain1 = detailedResponseChain, chain2 = chainWithHeading)\n",
    "\n",
    "output = parallelChain.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "# Output time reduced, even though 2 inputs are passed\n",
    "print(output['chain1'])\n",
    "print(\"\\n\\n\")\n",
    "print(output['chain2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d154b",
   "metadata": {},
   "source": [
    "### Runnable Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1254c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "firstPrompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ? /no_think\"),\n",
    "])\n",
    "\n",
    "# Chain 1\n",
    "firstResponseChain = firstPrompt | llm_qwen | StrOutputParser()\n",
    "\n",
    "followingPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                     Analyse the response & get me just the headings from the {response}.\n",
    "                                     Response should be in bullet points. Without explanation, but just the headings.\n",
    "                                     \"\"\")\n",
    "\n",
    "# Creating the RunnableLamda\n",
    "decide_llm = lambda response: llm_llama if len(str(response)) > 300 else llm_qwen\n",
    "\n",
    "llm_selector = RunnableLambda(decide_llm)\n",
    "\n",
    "#Chain 2\n",
    "chainWithHeading = {\"response\": firstResponseChain} | followingPrompt | llm_selector | StrOutputParser() | remove_any_think_block\n",
    "\n",
    "output = chainWithHeading.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585059cd",
   "metadata": {},
   "source": [
    "### Using @chain Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6bc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "firstPrompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are the {tool} expert\"),\n",
    "    (\"user\", \"What is the advantage of {action} the {tool} {article} {var} ? /no_think\"),\n",
    "])\n",
    "\n",
    "# Chain 1\n",
    "firstResponseChain = firstPrompt | llm_qwen | StrOutputParser()\n",
    "\n",
    "followingPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                     Analyse the response & get me just the headings from the {response}.\n",
    "                                     Response should be in bullet points.\n",
    "                                     \"\"\")\n",
    "\n",
    "# Using the @chain decorator to make `decide_llm` Runnable\n",
    "@chain\n",
    "def decide_llm(response):\n",
    "    response_text = str(response)\n",
    "    if len(response_text) > 300:\n",
    "        return llm_llama\n",
    "    return llm_qwen\n",
    "\n",
    "#Chain 2\n",
    "chainWithHeading = {\"response\": firstResponseChain} | followingPrompt | decide_llm | remove_any_think_block\n",
    "\n",
    "output = chainWithHeading.invoke({\"action\": \"running\", \"tool\": \"LLM\", \"article\": \"on\", \"var\": \"Local machine\"})\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvLangchain",
   "language": "python",
   "name": "myenvlangchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
