{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fdb1463",
   "metadata": {},
   "source": [
    "## RAG with PDF Data Extraction | Provide Context to LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422a963",
   "metadata": {},
   "source": [
    "### Load the ENV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4121907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "load_dotenv('./../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9818c1",
   "metadata": {},
   "source": [
    "### Load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48add113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U langchain-community pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f31b3",
   "metadata": {},
   "source": [
    "### 1. Extracting the PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf1 = \"./Dhanesh_EP_Letter_Nityo.pdf\"\n",
    "pdf2 = \"./Parveen_EP_Letter.pdf\"\n",
    "pdf3 = \"./Dhanesh_EP_Letter_NTT.pdf\"\n",
    "\n",
    "pdfFiles = [pdf1, pdf2, pdf3]\n",
    "\n",
    "documents = []\n",
    "\n",
    "for files in pdfFiles:\n",
    "    loader = PyPDFLoader(files)\n",
    "\n",
    "    \"\"\" \n",
    "    documents.append(loader.load()) would add each PDF document at a distinct index. \n",
    "    Meaning, documents[0] will have the 1st PDF document, documents[1] will have the 2nd PDF, & so on.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" \n",
    "    documents.extend(loader.load()) would store each PDF page at a distinct index in documents.\n",
    "    Meaning, documents[0] will have the 1st page of 1st PDF, & so on.\n",
    "    \"\"\"\n",
    "    documents.extend(loader.load())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69128834",
   "metadata": {},
   "source": [
    "### 2. Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=150, add_start_index=True)\n",
    "\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(len(all_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe54413b",
   "metadata": {},
   "source": [
    "### 3. Embedding the Splits into Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(base_url= \"http://localhost:11434\", model=\"qwen3:latest\", temperature=0.5)\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b1214",
   "metadata": {},
   "source": [
    "### 4. Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    persist_directory=\"./chroma_db\",  # Where to save data locally\n",
    "    embedding=embeddings,             # Embeddings created in step 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be82489",
   "metadata": {},
   "source": [
    "### 5. Retrieve from the Persistent Vector DataStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    persist_directory='./chroma_db', \n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "result = vector_store.similarity_search(\"Full name of Parveen\", k=5)\n",
    "\n",
    "for doc in result:\n",
    "    print(doc.metadata[\"source\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fd8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vector_store.similarity_search_with_score(\"Full name of Parveen?\", k=4)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a24aad",
   "metadata": {},
   "source": [
    "### 6. Retriever Interface in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec311f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}\n",
    ")\n",
    "\n",
    "retriever.batch(\n",
    "    [\n",
    "        \"What is full name of Parveen?\",\n",
    "        \"What is full name of Dhanesh?\",\n",
    "        \"Can Dhanesh enter Singapore with the IPA\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8ba8b0",
   "metadata": {},
   "source": [
    "### Manual Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd886019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# query = \"Which organisation is hiring Dhanesh\"\n",
    "# query = \"Which pass is issued to Dhanesh\"\n",
    "query = \"What is the full name of Parveen\"\n",
    "# query = \"What is the full name of Dhanesh\"\n",
    "# query = \"What do you know about PARVEEN?\"\n",
    "# query = \"Tell me genders of Dhanesh & Parveen\"\n",
    "# query = \"Who is the first Prime minister of India\"\n",
    "\n",
    "retrieved_doc = retriever.invoke(query)\n",
    "\n",
    "context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_doc])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\n",
    "    \"\"\"\n",
    "    You are an AI assistant. Make use of the following context fully to answer the question correctly.\n",
    "    If you do not know the answer, then tell that, I do not know.\n",
    "\n",
    "    \"context: {context} \\n\\n\"\n",
    "    \"question: {question} \\n\\n\"\n",
    "    \"AI Answer: \n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\": context_text, \"question\": query})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b461e",
   "metadata": {},
   "source": [
    "###  Data Retrieval using Langchain HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# query = \"Tell me genders of Dhanesh & Parveen\"\n",
    "# query = \"Which pass is issued to SHAIKH PARVEEN\"\n",
    "# query = \"What do you know about PARVEEN?\"\n",
    "# query = \"Which 2 organisations offered job to Dhanesh\"\n",
    "query = \"What is the full name of Parveen. It is mentioned in the letter.\"\n",
    "# query = \"Which country is Dhanesh allowed to travel\"\n",
    "# query = \"Who is the first Prime minister of India\"\n",
    "\n",
    "# https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=80099dc1-c38a-4ffb-b825-09b2ecbb562f\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\": context_text, \"question\": query})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e912c",
   "metadata": {},
   "source": [
    "### Data Retrieval using RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "custom_chain = RetrievalQA.from_chain_type(llm, retriever = retriever, return_source_documents = True)\n",
    "\n",
    "query = \"Full name of Dhanesh?\"\n",
    "\n",
    "response = custom_chain.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvLangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
