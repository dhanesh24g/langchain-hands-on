{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc0661e",
   "metadata": {},
   "source": [
    "### Configure LLM & Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"qwen3:latest\",\n",
    "    # model=\"deepseek-r1:8b\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=400\n",
    ")\n",
    "\n",
    "load_dotenv('./../.env')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64422eb5",
   "metadata": {},
   "source": [
    "### Installing a Community Tool (Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9817a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e92791",
   "metadata": {},
   "source": [
    "### Integrating Tool Response with LLM \n",
    "### (My POC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import re\n",
    "\n",
    "wiki_check = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=2))\n",
    "\n",
    "query = \"Which flight crashed in Ahmedabad in 2025 ?\"\n",
    "\n",
    "wiki_result = wiki_check.invoke(query)\n",
    "\n",
    "input_prompt = ChatPromptTemplate.from_template(\n",
    "\n",
    "    \"\"\"\n",
    "    You are an AI assistant. Use the following context to answer the question correctly.\n",
    "    Answer in a concise way & do not output the think tags.\n",
    "    If you don't know the answer, just say \"I don't know\".\n",
    "\n",
    "    \"context: {context}\"\n",
    "    \"question: {question}\"\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Making remove_think_block as a Runnable in order to chain it\n",
    "@chain\n",
    "def remove_think_block(text) -> str:\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "chain = input_prompt | llm | StrOutputParser() | remove_think_block\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"context\": wiki_result,\n",
    "    \"question\": query\n",
    "})\n",
    "\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d662345",
   "metadata": {},
   "source": [
    "### Creating Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"Result: Addition of 2 numbers\"\n",
    "    return a+b\n",
    "\n",
    "@tool\n",
    "def subtract_numbers(a: int, b: int) -> int:\n",
    "    \"Result: Subtraction of 2 numbers\"\n",
    "    return a-b\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(a: int, b: float) -> float:\n",
    "    \"Result: Multiplication of 2 numbers\"\n",
    "    return a*b\n",
    "\n",
    "\n",
    "add_numbers.run({\"a\":133, \"b\":2212})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tools = [wiki_check, add_numbers, subtract_numbers, multiply_numbers]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools=custom_tools)\n",
    "\n",
    "question = \"Who is the winner of the ICC Champions Trophy 2025 ?\"\n",
    "# question = \"What is the multiplication on 2.5 & 4.8 ?\"\n",
    "# question = \"What is the division of 8 & 4 ?\"\n",
    "# question = \"I have 10 Dollars, I gave 6 Dollars to mother. How many remain with me ?\"\n",
    "# question = \"I have 50K Dollars in my account. I got 56K dollars as bonus. How much is my account balance ?\"\n",
    "# question = \"What is the latest Air India flight crash in Ahmedabad ?\"\n",
    "\n",
    "response = llm_with_tools.invoke(question)\n",
    "\n",
    "print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c7321",
   "metadata": {},
   "source": [
    "### Executing the Custom Tool Call & Integrate with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# question = \"Who won IPL 2025 ?\"\n",
    "# question = \"Latest Air India flight crash ?\"\n",
    "question = \"My current salary is 10K, now I got 15 percent hike. What is my new salary ?\"\n",
    "# question = \"Who is the winner of the ICC Champions Trophy 2025 ?\"\n",
    "\n",
    "\n",
    "# Get the 1st component -> Question to be asked (HumanMessage)\n",
    "input_for_llm = [HumanMessage(question + \"/no_think\")]\n",
    "\n",
    "# Generate the AI Message with tool call (AIMessage)\n",
    "ai_message = llm_with_tools.invoke(question)\n",
    "\n",
    "input_for_llm.append(ai_message)\n",
    "\n",
    "list_of_tools = {tool.name: tool for tool in custom_tools}\n",
    "\n",
    "for tool_call in ai_message.tool_calls:\n",
    "    tool_name = tool_call['name'].lower()\n",
    "    \n",
    "    tool_to_execute = list_of_tools[tool_name]\n",
    "    \n",
    "    # Generate the context by using the AI Message & the Human Message -> (ToolMessage)\n",
    "    executed_tool = tool_to_execute.invoke(tool_call)\n",
    "    \n",
    "    # Append the last leg before calling the LLM to generate the output\n",
    "    # HumanMessage, AIMessage & ToolMessage make the input_for_llm complete\n",
    "    input_for_llm.append(executed_tool)\n",
    "\n",
    "\n",
    "# Integrating the Tool call (HumanMessage, AIMessage & ToolMessage) to LLM\n",
    "llm_response = llm_with_tools.invoke(input_for_llm)\n",
    "print(llm_response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvLangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
